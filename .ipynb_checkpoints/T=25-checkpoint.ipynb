{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35458121",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "import os\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "from Modules.training_functions import *\n",
    "from Modules.pennylane_functions import *\n",
    "\n",
    "# if gpu available, set device to gpu\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(\"Using the GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"WARNING: Could not find GPU, using the CPU\")\n",
    "ld_dim=8\n",
    "T=10\n",
    "# load dataset\n",
    "mnist_images0 = np.load(f'Data/dataset_ld_{ld_dim}_0.npy')\n",
    "mnist_images1 = np.load(f'Data/dataset_ld_{ld_dim}_1.npy')\n",
    "\n",
    "mnist_images =np.concatenate((mnist_images0, mnist_images1), axis = 0)\n",
    "print(np.shape(mnist_images))\n",
    "\n",
    "np.random.shuffle(mnist_images)\n",
    "mnist_images = torch.tensor(mnist_images).to(device)\n",
    "\n",
    "# make dataloader\n",
    "data_loader = torch.utils.data.DataLoader(mnist_images, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "qc_array=np.array([0,2])\n",
    "min_array=np.array([0.1,0.05,0.01,0.08])\n",
    "layer_array=np.array([5,10,20,50])\n",
    "print(NUM_QUBITS)\n",
    "print(T)\n",
    "for layer_indx in range(len(layer_array)):\n",
    "    n_layer=layer_array[layer_indx]\n",
    "    for q_indx in range(len(qc_array)):\n",
    "        qc=qc_array[q_indx]\n",
    "        for min_indx in range(len(min_array)):\n",
    "            min_b=min_array[min_indx]\n",
    "\n",
    "            betas      = np.insert(np.linspace(10e-8,min_b, T), 0, 0)\n",
    "            print(np.shape(betas))\n",
    "            alphas     = 1 - betas\n",
    "            alphas_bar = np.cumprod(alphas)\n",
    "            pi         = math.pi\n",
    "            betas      = torch.tensor(betas).float().to(device)\n",
    "            alphas     = torch.tensor(alphas).float().to(device)\n",
    "            alphas_bar = torch.tensor(alphas_bar).float().to(device)\n",
    "            theta_1    = Variable(torch.rand((n_layer*3*NUM_QUBITS+n_layer*3*(NUM_QUBITS)), device = device), requires_grad=True)\n",
    "            optimizer = torch.optim.Adam([theta_1], lr = LEARNING_RATE)\n",
    "            scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = SCHEDULER_PATIENCE, gamma = SCHEDULER_GAMMA, verbose = False)\n",
    "            trained_thetas_1 = []\n",
    "            loss_history = []\n",
    "            best_loss = 1e10\n",
    "\n",
    "            for epoch in range(NUM_EPOCHS):\n",
    "                print(epoch)\n",
    "\n",
    "                t0 = time.time()\n",
    "                num_batch=0\n",
    "                tot_loss=0\n",
    "\n",
    "                for image_batch in data_loader:\n",
    "\n",
    "                    # extract batch of random times and betas\n",
    "                    t = torch.randint(0, T, size = (BATCH_SIZE, ), device=device)\n",
    "                    betas_batch = betas[t].to(device)\n",
    "                    alphas_batch=alphas_bar[t].to(device)\n",
    "\n",
    "                    # assemble input at t add noise (t+1)\n",
    "                    target_batch = assemble_input(image_batch, t, alphas_bar,ld_dim ,device)\n",
    "                    input_batch  = noise_step(target_batch, t+1, betas,ld_dim, device)\n",
    "                    target_batch = target_batch / torch.norm(target_batch, dim = 1).view(-1, 1)\n",
    "                    input_batch  = input_batch / torch.norm(input_batch, dim = 1).view(-1, 1)\n",
    "                    #zero = torch.zeros(128, 256*7).to(device)\n",
    "\n",
    "                    # concatenate the two tensors along the second dimension\n",
    "                    #input_batch = torch.cat((input_batch, zero), dim=1)\n",
    "                    #target_batch = torch.cat((target_batch, zero), dim=1)\n",
    "                    # Feed to circuit, compute the loss and update the weights\n",
    "                    num_batch+=1\n",
    "                    loss = loss_fn_aq(qc,theta_1,n_layer, input_batch, target_batch)\n",
    "                    tot_loss+=loss.item()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                # append parameters and print loss\n",
    "                trained_thetas_1.append(theta_1.cpu().clone().detach().numpy())\n",
    "\n",
    "                loss_history.append(tot_loss/num_batch)\n",
    "                if loss.item()< best_loss:\n",
    "                    best_loss=loss.item()\n",
    "\n",
    "                # implement learning rate scheduler\n",
    "                scheduler.step()\n",
    "\n",
    "\n",
    "            # print every epoch\n",
    "                print(f'T={T} Epoch: {epoch+1}/{NUM_EPOCHS} - Loss: {loss.item():.4f} b_loss={best_loss:.4f} - T: {time.time()-t0:.2f}s/epoch ,tempo_previto={((time.time()-t0)*(NUM_EPOCHS-1-epoch+NUM_EPOCHS*(len(qc_array)-q_indx-1)+NUM_EPOCHS*len(qc_array)*(len(min_array)-min_indx-1)+NUM_EPOCHS*len(qc_array)*len(min_array)*(len(layer_array)-layer_indx-1)))/60:.2f} min{min_b} nl{n_layer}')\n",
    "                #print(f'T={T} Epoch: {epoch+1}/{NUM_EPOCHS} - Loss: {loss.item():.4f} b_loss={best_loss:.4f} - T: {time.time()-t0:.2f}s/epoch ,tempo_previto={(((NUM_EPOCHS-1-epoch+NUM_EPOCHS*(len(qc_array)-q_indx-1)+NUM_EPOCHS*len(qc_array)*(len(min_array)-min_indx-1)+NUM_EPOCHS*len(qc_array)*len(min_array)*(len(layer_array)-layer_indx-1)))):.2f} min{min_b} nl{n_layer}')\n",
    "                \n",
    "            np.save(f'thetas_T{T}_nl{n_layer}_min{min_b}_qc{qc}_{Q_ANCILLA}_ld{ld_dim}.npy',trained_thetas_1)\n",
    "            np.save(f'loss__T{T}_nl{n_layer}_min{min_b}_qc{qc}_ancilla{Q_ANCILLA}_ld{ld_dim}.npy',loss_history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
